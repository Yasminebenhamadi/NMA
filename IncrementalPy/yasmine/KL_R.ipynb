{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasminebenhamadi/NMA/blob/main/IncrementalPy/yasmine/KL_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTHOvF1ZIpCc",
        "outputId": "283b7b55-8e6d-4524-8447-88b9ecd8d381"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\benhamya\\AppData\\Local\\Temp\\ipykernel_240\\3443411012.py:19: DeprecationWarning: Please use `kruskal` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import kruskal\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn import mixture\n",
        "import random\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import scipy.stats\n",
        "from sklearn.datasets import make_spd_matrix,make_blobs\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pair_confusion_matrix, davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
        "from numpy import linalg as la\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats.stats import kruskal\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.stats import zscore\n",
        "import re\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-dark')\n",
        "plt.rcParams['figure.figsize']=14,6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mJstH7mK2St"
      },
      "source": [
        "### utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysM9_IyCJ9YM"
      },
      "outputs": [],
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lKOGsbgJ5v5"
      },
      "outputs": [],
      "source": [
        "def visualize_components(component1, component2, labels, show=True):\n",
        "  \"\"\"\n",
        "  Plots a 2D representation of the data for visualization with categories\n",
        "  labelled as different colors.\n",
        "\n",
        "  Args:\n",
        "    component1 (numpy array of floats) : Vector of component 1 scores\n",
        "    component2 (numpy array of floats) : Vector of component 2 scores\n",
        "    labels (numpy array of floats)     : Vector corresponding to categories of\n",
        "                                         samples\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  cmap = plt.cm.get_cmap('tab10')\n",
        "  plt.scatter(x=component1, y=component2, c=labels, cmap=cmap)\n",
        "  plt.xlabel('Component 1')\n",
        "  plt.ylabel('Component 2')\n",
        "  plt.colorbar(ticks=range(10))\n",
        "  plt.clim(-0.5, 9.5)\n",
        "  if show:\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah6PDyyLwNac"
      },
      "outputs": [],
      "source": [
        "def gmm_bic_score(estimator, X):\n",
        "    \"\"\"Callable to pass to GridSearchCV that will use the BIC score.\"\"\"\n",
        "    # Make it negative since GridSearchCV expects a score to maximize\n",
        "    return -estimator.bic(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqrkUkPpWBqN"
      },
      "outputs": [],
      "source": [
        "def ari(labels_true,labels_pred): \n",
        "    '''safer implementation of ari score calculation'''\n",
        "    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
        "    tn=int(tn)\n",
        "    tp=int(tp)\n",
        "    fp=int(fp)\n",
        "    fn=int(fn)\n",
        "\n",
        "    # Special cases: empty data or full agreement\n",
        "    if fn == 0 and fp == 0:\n",
        "        return 1.0\n",
        "\n",
        "    return 2. * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) +\n",
        "                                       (tp + fp) * (fp + tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU2J9flvkz1R"
      },
      "outputs": [],
      "source": [
        "def f1_score(labels_true,labels_pred): \n",
        "    '''safer implementation of ari score calculation'''\n",
        "    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
        "    tn=int(tn)\n",
        "    tp=int(tp)\n",
        "    fp=int(fp)\n",
        "    fn=int(fn)\n",
        "\n",
        "    precision= tp/(tp+fp)\n",
        "    recall= tp/(tp+fn)\n",
        "\n",
        "    # Special cases: empty data or full agreement\n",
        "    if fn == 0 and fp == 0:\n",
        "        return 1.0\n",
        "\n",
        "    return 2. * precision * recall / (precision+recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44mhc8DKh4b0"
      },
      "outputs": [],
      "source": [
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oXsB1KuSIFn"
      },
      "outputs": [],
      "source": [
        "def min_max(data):\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(data)\n",
        "  return scaler.transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-SyZIKXJGyv"
      },
      "source": [
        "## Ploting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHpKl9ZuK0DO"
      },
      "outputs": [],
      "source": [
        "def plot_bivariate_data(X, title):\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax = fig.add_subplot(gs[:, 1])\n",
        "  ax.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
        "           markeredgewidth=0)\n",
        "  plt.xlabel('Feature 1')\n",
        "  plt.ylabel('Feature 2')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Wg1jbguVOO"
      },
      "outputs": [],
      "source": [
        "def plot_contours(data, means, covs,labels, title, xa=[-12,12],ya=[-12,12]):\n",
        "    \"\"\"visualize the gaussian components over the data\"\"\"\n",
        "    plt.figure()\n",
        "    cmap = plt.cm.get_cmap('tab10')\n",
        "    plt.scatter(data[:, 0], data[:, 1],c=labels, cmap=cmap, s=40 ,alpha=0.4)\n",
        "    \n",
        "\n",
        "    delta = 0.025\n",
        "    if len(means)>0:\n",
        "      k = means.shape[0]\n",
        "    else:\n",
        "      k=0\n",
        "    x = np.arange(xa[0], xa[1], delta)\n",
        "    y = np.arange(ya[0], ya[1], delta)\n",
        "    x_grid, y_grid = np.meshgrid(x, y)\n",
        "    coordinates = np.array([x_grid.ravel(), y_grid.ravel()]).T\n",
        "\n",
        "    col = ['cyan', 'red', 'indigo','blue','white']\n",
        "    for i in range(k):\n",
        "        mean = means[i]\n",
        "        cov = covs[i]\n",
        "        z_grid = multivariate_normal(mean, cov).pdf(coordinates).reshape(x_grid.shape)\n",
        "        plt.contour(x_grid, y_grid, z_grid, colors = col[i])\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsRmlQCJyLh"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCBwEdYCSALl"
      },
      "outputs": [],
      "source": [
        "def generate_bivariate_data(nbr_components=1,min_mean=[-10],max_mean=[10],scale=[], size=1000):\n",
        "  data = np.zeros((nbr_components,size,2))\n",
        "  loc = [i*5 for i in range(nbr_components)]\n",
        "  loc1 = np.random.uniform(low=min_mean,high=max_mean,size=nbr_components)\n",
        "  if len(scale)==0:\n",
        "    scale= make_spd_matrix(n_dim=2)\n",
        "  means = []\n",
        "  for i in range(nbr_components):\n",
        "    mean = [loc[i],loc1[i]]\n",
        "    s=np.random.multivariate_normal(mean=mean, cov=scale, size=size)\n",
        "    means.append(mean)\n",
        "    data[i] = s\n",
        "  np.random.shuffle(data)\n",
        "  return data[0], means,scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2trp_Ve3yPnr"
      },
      "outputs": [],
      "source": [
        "def generate_bivariate_component(mean,scale,size=1000):\n",
        "  data = np.zeros((1,size,2))\n",
        "  if len(scale)==0:\n",
        "    scale= make_spd_matrix(n_dim=2)\n",
        "  data=np.random.multivariate_normal(mean=mean, cov=scale, size=size)\n",
        "  np.random.shuffle(data)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alavfe7jNClq"
      },
      "outputs": [],
      "source": [
        "def generate_bivariate_data_overlap(nbr_components=2,mean=[0,0],var_1=1,var_2=10, size=1000):\n",
        "  data = np.zeros((nbr_components,size,2))\n",
        "  scale1= [[var_1,random.uniform(0, 1.5)],[random.uniform(0, 1.5),var_2]]\n",
        "  scale2= [[var_2,random.uniform(0, 1.5)],[random.uniform(0, 1.5),var_1]]\n",
        "  for i in range(nbr_components):\n",
        "    s=np.random.multivariate_normal(mean=mean, cov=scale1 if i%2==1 else scale2, size=size)\n",
        "    data[i] = s\n",
        "    \n",
        "\n",
        "  np.random.shuffle(data)\n",
        "  return np.concatenate((data[0], data[1])), data[0],data[1],[mean,scale1,scale2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_8ExRVGJ8g_"
      },
      "source": [
        "# KL Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpZkJ6rYBTuH"
      },
      "outputs": [],
      "source": [
        "def kl_mvn(m0, S0, m1, S1):\n",
        "    \"\"\"\n",
        "    Kullback-Liebler divergence from Gaussian pm,pv to Gaussian qm,qv.\n",
        "    Also computes KL divergence from a single Gaussian pm,pv to a set\n",
        "    of Gaussians qm,qv.\n",
        "    \n",
        "\n",
        "    From wikipedia\n",
        "    KL( (m0, S0) || (m1, S1))\n",
        "         = .5 * ( tr(S1^{-1} S0) + log |S1|/|S0| + \n",
        "                  (m1 - m0)^T S1^{-1} (m1 - m0) - N )\n",
        "    \"\"\"\n",
        "    # store inv diag covariance of S1 and diff between means\n",
        "    N = m0.shape[0]\n",
        "    iS1 = np.linalg.inv(S1)\n",
        "    diff = m1 - m0\n",
        "\n",
        "    # kl is made of three terms\n",
        "    tr_term   = np.trace(iS1 @ S0)\n",
        "    det_term  = np.log(np.linalg.det(S1)/(np.linalg.det(S0)+1e-6)) #np.sum(np.log(S1)) - np.sum(np.log(S0))\n",
        "    quad_term = diff.T @ np.linalg.inv(S1) @ diff #np.sum( (diff*diff) * iS1, axis=1)\n",
        "    return .5 * (tr_term + det_term + quad_term - N) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt2XkhWJFQZg"
      },
      "outputs": [],
      "source": [
        "def KL_matrix(m0,S0,m1,S1):\n",
        "  k0=m0.shape[0]\n",
        "  k1=m1.shape[0]\n",
        "  M=np.zeros((k0,k1))\n",
        "  for i in range(k0):\n",
        "    for j in range(k1):\n",
        "      M[i,j]=kl_mvn(m0[i],S0[i], m1[j], S1[j])\n",
        "  return M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aBOiUl1KW5O"
      },
      "source": [
        "# Incremental EM for GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCUGtV_cUrps"
      },
      "outputs": [],
      "source": [
        "def trainGMM_lot(data_x,num,n_components,dim, random_state, covariance_type='full',max_iter = 100,tol=1e-03):\n",
        "  w = []\n",
        "  m = []\n",
        "  covs = []\n",
        "  gmm = [GMM(n_components=n_components, covariance_type=covariance_type, random_state=random_state, max_iter=max_iter,tol=tol) for i in range(num)]\n",
        "  for i in range(num):\n",
        "    gmm[i].fit(data_x[i])\n",
        "    w.append(gmm[i].weights_)\n",
        "    m.append(gmm[i].means_)\n",
        "    covs.append(gmm[i].covariances_)\n",
        "  w = np.array(w)\n",
        "  m = np.array(m)\n",
        "  covs = np.array(covs)\n",
        "  if num==1:\n",
        "    w = w.reshape(n_components)\n",
        "    m = m.reshape(n_components,dim)\n",
        "    covs = covs.reshape(n_components,dim,dim)\n",
        "  return w,m,covs,gmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_h9OBLGe7O0"
      },
      "outputs": [],
      "source": [
        "def trainGMM(data_x,n_components,dim, random_state,covariance_type='full',max_iter = 100,tol=1e-03):\n",
        "  gmm = GMM(n_components=n_components, covariance_type=covariance_type, random_state=random_state, max_iter=max_iter,tol=tol)\n",
        "  gmm.fit(data_x)\n",
        "  return gmm.weights_,gmm.means_,gmm.covariances_,gmm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHwzB7XC-H_x"
      },
      "outputs": [],
      "source": [
        "def merge_two(n0,w0,m0,s0,n1,w1,m1,s1):\n",
        "    new_mean=(n0*w0*m0+n1*w1*m1)/(n0*w0+n1*w1)\n",
        "    new_weight=(n0*w0+n1*w1)/(n0+n1)\n",
        "    s1=(n0*w0*s0+n1*w1*s1)/(n0*w0+n1*w1)\n",
        "    sw=n0*w0*np.outer(np.transpose(m0),m0)+n1*w1*np.outer(np.transpose(m1),m1)\n",
        "    s2=sw/(n0*w0+n1*w1)\n",
        "    sub3=np.outer(np.transpose(new_mean),new_mean)\n",
        "    new_cov=s1+s2-sub3\n",
        "    return new_weight,new_mean,new_cov\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktDB41fKNeyz"
      },
      "outputs": [],
      "source": [
        "def delete_e(w,i):\n",
        "  liste = w.tolist()\n",
        "  liste.pop(i)\n",
        "  return np.array(liste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I_vN-14O9_D"
      },
      "outputs": [],
      "source": [
        "def delete_all(w,m,s,i):\n",
        "  w0=delete_e(w,i)\n",
        "  m0=delete_e(m,i)\n",
        "  s0=delete_e(s,i)\n",
        "  return w0,m0,s0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxn90z6OzNP_"
      },
      "outputs": [],
      "source": [
        "def merge(n0,w0,m0,s0,n1,w1,m1,s1):\n",
        "  KL=KL_matrix(m0,s0,m1,s1)\n",
        "  new_weights=[]\n",
        "  new_means=[]\n",
        "  new_covs=[]\n",
        "  for k in range(KL.shape[0]):\n",
        "    ij_min = np.unravel_index(KL.argmin(), KL.shape)\n",
        "    i,j=ij_min\n",
        "    new_weight,new_mean,new_cov = merge_two(n0,w0[i],m0[i],s0[i],n1,w1[j],m1[j],s1[j])\n",
        "    new_weights.append(new_weight)\n",
        "    new_means.append(new_mean)\n",
        "    new_covs.append(new_cov)\n",
        "    KL= np.delete(KL, ij_min[0], 0)\n",
        "    KL= np.delete(KL, ij_min[1], 1)\n",
        "    w0,m0,s0=delete_all(w0,m0,s0,i)\n",
        "    w1,m1,s1=delete_all(w1,m1,s1,j)\n",
        "  return np.array(new_weights),np.array(new_means),np.array(new_covs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGBIsbgzL1Vh"
      },
      "outputs": [],
      "source": [
        "def incGMM(data,dim, n_components, increments_number, random_state, covariance_type='full',max_iter = 100,tol=1e-03, true_lables=[], incPrint=True):\n",
        "  size_increments=int(len(data)/increments_number)\n",
        "  clus_increment_size=int(size_increments/n_components)\n",
        "  assignments=[]\n",
        "  increments=[]\n",
        "  weights=[]\n",
        "  means=[]\n",
        "  inc_labels=[]\n",
        "  covariances=[]\n",
        "  n0=0\n",
        "  gmm = 0\n",
        "  for i in range(increments_number):\n",
        "      s=[i for j in range(size_increments)]\n",
        "      inc_labels.append(s)\n",
        "      if i == (increments_number - 1):\n",
        "        inc = data[i*size_increments:data.shape[0],:]\n",
        "      else:\n",
        "        inc = data[i*size_increments:size_increments*(i+1),:]\n",
        "      increments.append(inc)\n",
        "      w,m,covs,gmm = trainGMM(inc,n_components,dim, covariance_type='full',random_state=random_state,max_iter = 100,tol=1e-03)\n",
        "      n1=len(inc)\n",
        "      if len(weights)>0:\n",
        "        w,m,covs=merge(n0,w,m,covs,n1,weights,means,covariances)\n",
        "      weights,means,covariances=w,m,covs\n",
        "      n0=n0+n1\n",
        "      if incPrint and (len(true_lables)>0) and data.shape[1]==2:\n",
        "        plot_contours(inc,means,covariances,true_lables[i*size_increments:size_increments*(i+1)], \"increments_true\")\n",
        "        plt.savefig(\"inc_true{}.png\".format(i))\n",
        "  if incPrint and data.shape[1]==2:\n",
        "    increments, inc_labels = np.array(increments).reshape(data.shape[0],dim) , np.array(inc_labels).reshape(data.shape[0])\n",
        "    plot_contours(increments,[],[],inc_labels, \"increments_partition\")\n",
        "  return np.array(weights), np.array(means), np.array(covariances), gmm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVq_mcUuKoS1"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCewyBVqSaGR"
      },
      "outputs": [],
      "source": [
        "def getFinalGmm(data, means, covariances, weights, true_labels = [], plot=True):\n",
        "    gmm=GMM(n_components=weights.shape[0],covariance_type='full',max_iter=1)\n",
        "    gmm.means_=means\n",
        "    gmm.covariances_=covariances\n",
        "    gmm.weights_=weights\n",
        "    precisions_cholesky = np.linalg.inv(la.cholesky(covariances))\n",
        "    gmm.precisions_cholesky_= np.array([np.transpose(p) for p in precisions_cholesky])\n",
        "    assign = gmm.predict(data)\n",
        "    if plot and len(true_labels>0) and data.shape[1]==2:\n",
        "        plot_contours(data,means,covariances,assign, \"increments\")\n",
        "    return gmm, assign\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU_mMTLDWKBU"
      },
      "outputs": [],
      "source": [
        "def compare(data,dim, n_components, increments_number, random_state, true_labels = [], printB=True, max_iter = 100,tol=1e-03, covariance_type='full'):\n",
        "    weights, means, covariances, gm = incGMM(data,dim, n_components=n_components, increments_number=increments_number, covariance_type=covariance_type,random_state=random_state,max_iter = max_iter,tol=tol, true_lables=true_labels, incPrint=printB)\n",
        "    gmm_inc, assign_inc = getFinalGmm(data, means, covariances, weights, true_labels = true_labels, plot=printB)\n",
        "    all_gmm=GMM(n_components=n_components, covariance_type=covariance_type,max_iter = max_iter,tol=tol)\n",
        "    all_gmm.fit(data)\n",
        "    assign_all = all_gmm.predict(data)\n",
        "    ############\n",
        "    partial_gmm=GMM(n_components=n_components, covariance_type=covariance_type,max_iter = max_iter,tol=tol)\n",
        "    #print((len(data)/increments_number))\n",
        "    partial_gmm.fit(data[0:int(len(data)/increments_number)])\n",
        "    assign_part = partial_gmm.predict(data)\n",
        "\n",
        "    all_metrics = [all_gmm.score(data), partial_gmm.score(data), gmm_inc.score(data)]\n",
        "    all_metrics.extend([davies_bouldin_score(data,true_labels), davies_bouldin_score(data,assign_all), davies_bouldin_score(data,assign_part), davies_bouldin_score(data,assign_inc)])\n",
        "    all_metrics.extend([calinski_harabasz_score(data,true_labels), calinski_harabasz_score(data,assign_all), calinski_harabasz_score(data,assign_part), calinski_harabasz_score(data,assign_inc)])\n",
        "    all_metrics.extend([ari(true_labels.flatten(),assign_all.flatten()), ari(true_labels.flatten(),assign_part.flatten()), ari(true_labels.flatten(),assign_inc.flatten())])\n",
        "    all_metrics.extend([f1_score(true_labels.flatten(),assign_all.flatten()), f1_score(true_labels.flatten(),assign_part.flatten()), f1_score(true_labels.flatten(),assign_inc.flatten())])\n",
        "\n",
        "    return all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHKNBiHvvYjC"
      },
      "outputs": [],
      "source": [
        "def getIC(data,dim, n_components, increments_number, random_state, true_labels = [], printB=True, max_iter = 100,tol=1e-03, covariance_type='full'):\n",
        "  weights, means, covariances, gm = incGMM(data,dim, n_components=n_components, increments_number=increments_number, covariance_type=covariance_type,random_state=random_state,max_iter = max_iter,tol=tol, true_lables=true_labels, incPrint=printB)\n",
        "  gmm_inc, assign_inc = getFinalGmm(data, means, covariances, weights, true_labels = true_labels, plot=printB)\n",
        "  \n",
        "  all_gmm=GMM(n_components=n_components, covariance_type=covariance_type,max_iter = max_iter,tol=tol)\n",
        "  all_gmm.fit(data)\n",
        "  \n",
        "  return gmm_inc.bic(data), gmm_inc.aic(data), all_gmm.aic(data), all_gmm.bic(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCdYiNLeBaer"
      },
      "outputs": [],
      "source": [
        "def kmeans_reorganize(toute,number_comp,increments_number, random_state):\n",
        "    means = KMeans(init=\"k-means++\", n_clusters=number_comp, n_init=4, random_state=random_state)\n",
        "    kmeans.fit(toute)\n",
        "    l=kmeans.predict(toute)\n",
        "    true_labels_k=np.unique(l)\n",
        "    clusters=[]\n",
        "    for j in true_labels_k:\n",
        "        cluster=[toute[i] for i in range(len(toute)) if l[i]==j]\n",
        "\n",
        "        clusters.append(cluster)\n",
        "    clusters_sizes=np.unique(l,return_counts=True)\n",
        "    clusters_sizes=clusters_sizes[1]\n",
        "    increment_size=int(sum(clusters_sizes)//increments_number)\n",
        "    sizes_cluster_per_increment=clusters_sizes//increments_number\n",
        "    trainingDS=[]#clusters with different sizes\n",
        "    for i in range(increments_number):\n",
        "        for j in range(number_comp):\n",
        "            ind=true_labels_k[j]\n",
        "            if j==0:\n",
        "               r0=clusters[j][i*sizes_cluster_per_increment[j]:(i+1)*sizes_cluster_per_increment[j]]\n",
        "               r1=r0\n",
        "            else:\n",
        "               r2=clusters[j][i*sizes_cluster_per_increment[j]:(i+1)*sizes_cluster_per_increment[j]]\n",
        "               r=np.concatenate((r1,r2),axis=0)\n",
        "\n",
        "               r1=r\n",
        "        trainingDS.append(r)\n",
        "    training=np.array(trainingDS)\n",
        "    training=training.reshape(training.shape[0]*training.shape[1],training.shape[2])\n",
        "    return training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RM3Wc5NRA6S"
      },
      "source": [
        "## R generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vQy5RIRRF2h"
      },
      "outputs": [],
      "source": [
        "def read_info_file(path):\n",
        "    words = [\"sepVal\", \"Number of clusters\", \"Number of dimensions\", \"Number of data points\", \"Number of outliers\"]\n",
        "    N, dim, number_comp, sepVal, outliers = (0,0,0,0,0)\n",
        "    with open(path, 'r') as fp:\n",
        "        # read all lines using readline()\n",
        "        lines = fp.readlines()\n",
        "        for row in lines:\n",
        "            for word in words:\n",
        "                if row.find(word) != -1:\n",
        "                    x = row.split(' ')[-1]\n",
        "                    if word == words[0]:\n",
        "                        sepVal=float(x)\n",
        "                    elif word == words[1]:\n",
        "                        number_comp=int(x)\n",
        "                    elif word == words[2]:\n",
        "                        dim=int(x)\n",
        "                    elif word == words[3]:\n",
        "                        N=int(x)\n",
        "                    else:\n",
        "                        outliers=float(x)\n",
        "                        \n",
        "    return N, dim, number_comp, sepVal, outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4fLLiddRdYG",
        "outputId": "8909acd5-1e79-4787-8012-6855986df3ed",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:/Users/benhamya/PFE/Rdatasets/datasets/balanced/dimBig/6/big-20000-0-300-10.mat\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "mainDir=\"C:/Users/benhamya/PFE/Rdatasets/datasets/balanced/\"\n",
        "datasets=[name for name in os.listdir(mainDir) if os.path.isdir(os.path.join(mainDir, name))]\n",
        "datasets=np.sort(datasets)\n",
        "i=0\n",
        "with open(mainDir+'results_dimBig6.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    header=[\"DatasetNumber\", \"size\", \"dim\", \"Percentage of increment\", \"cluster size\", \"number of clusters\", \"sepVal\", \"Percentage of outliers\", \"loglikelihood all\", \"loglikelihood part\", \"loglikelihood inc\", \"DBS true\", \"DBS all\", \"DBS part\", \"DBS inc\", \"CHS true\", \"CHS all\", \"CHS part\", \"CHS inc\", \"ARI all\", \"ARI part\", \"ARI inc\", \"F1 all\", \"F1 part\", \"F1 inc\"]\n",
        "    writer.writerow(header)\n",
        "    for dataset in datasets:\n",
        "        dataset_dir=mainDir+dataset+\"/\"\n",
        "        folder = os.listdir(dataset_dir)\n",
        "        for s in folder:\n",
        "            data_file_chemin = dataset_dir+s+\"/\"\n",
        "            files =os.listdir(data_file_chemin)\n",
        "            if(len(files)>0):\n",
        "                data_file =  data_file_chemin+[f for f in files if f.endswith(\".mat\")][0]\n",
        "                print(data_file)\n",
        "                labels_file =data_file_chemin+ [f for f in files if f.endswith(\".mem\")][0]\n",
        "                info_file = data_file_chemin+[f for f in files if (f.endswith(\".log\") and not f.endswith(\"info.log\"))][0]\n",
        "                N, dim, number_comp, sepVal, outliers = read_info_file(info_file)\n",
        "\n",
        "                data=pd.read_csv(data_file,sep=\" \",header=None)\n",
        "                data_labels=pd.read_csv(labels_file,header=None)\n",
        "                data_labels.rename(columns = {0:2}, inplace = True)\n",
        "                toute=pd.concat([data,data_labels],axis=1)\n",
        "                # sctt_plt = sns.scatterplot(data=toute, x=0, y=1, hue=2)\n",
        "                # fig = sctt_plt.get_figure()\n",
        "                # fig.savefig(dataset_dir+\"out.png\")\n",
        "                # plt.clf()\n",
        "                i=i+1\n",
        "                toute=pd.concat([data,data_labels],axis=1)\n",
        "                toute=toute.dropna()\n",
        "                toute=toute.to_numpy()\n",
        "                np.unique(data_labels.to_numpy(),return_counts=True)\n",
        "                labels_true=data_labels.to_numpy().reshape(-1)\n",
        "                data=data.to_numpy()\n",
        "                data=min_max(data)\n",
        "                #***********************************************GMM***********************************************\n",
        "                number_tries=5\n",
        "                for i in [2,3,4,10,20]:\n",
        "                    print(i)\n",
        "                    metrics = np.zeros(len(header)-8)\n",
        "                    for t in range(number_tries):\n",
        "                        metrics_i = compare(data, dim=dim, random_state=t, n_components=number_comp, increments_number=i, true_labels = labels_true, printB=False, max_iter = 100,tol=1e-03)\n",
        "                        metrics = metrics+metrics_i\n",
        "                    metrics = metrics/number_tries\n",
        "                    info = [dataset, N, dim, 100/i, int(N/number_comp), number_comp, sepVal, outliers]\n",
        "                    info.extend(metrics)\n",
        "                    writer.writerow(info)\n",
        "\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeYgAkyLBaer"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}