{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasminebenhamadi/NMA/blob/main/Python/KL/diag/merge_end/merging_strategy_repeat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTHOvF1ZIpCc",
        "outputId": "638627ec-a931-439b-dbff-0307f6f7b295"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_12317/3443411012.py:19: DeprecationWarning: Please use `kruskal` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import kruskal\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn import mixture\n",
        "import random\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import scipy.stats\n",
        "from sklearn.datasets import make_spd_matrix,make_blobs\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pair_confusion_matrix, davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
        "from numpy import linalg as la\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats.stats import kruskal\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.stats import zscore\n",
        "import re\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-dark')\n",
        "plt.rcParams['figure.figsize']=14,6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mJstH7mK2St"
      },
      "source": [
        "### utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysM9_IyCJ9YM"
      },
      "outputs": [],
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lKOGsbgJ5v5"
      },
      "outputs": [],
      "source": [
        "def visualize_components(component1, component2, labels, show=True):\n",
        "  \"\"\"\n",
        "  Plots a 2D representation of the data for visualization with categories\n",
        "  labelled as different colors.\n",
        "\n",
        "  Args:\n",
        "    component1 (numpy array of floats) : Vector of component 1 scores\n",
        "    component2 (numpy array of floats) : Vector of component 2 scores\n",
        "    labels (numpy array of floats)     : Vector corresponding to categories of\n",
        "                                         samples\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  cmap = plt.cm.get_cmap('tab10')\n",
        "  plt.scatter(x=component1, y=component2, c=labels, cmap=cmap)\n",
        "  plt.xlabel('Component 1')\n",
        "  plt.ylabel('Component 2')\n",
        "  plt.colorbar(ticks=range(10))\n",
        "  plt.clim(-0.5, 9.5)\n",
        "  if show:\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqrkUkPpWBqN"
      },
      "outputs": [],
      "source": [
        "def ari(labels_true,labels_pred): \n",
        "    '''safer implementation of ari score calculation'''\n",
        "    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
        "    tn=int(tn)\n",
        "    tp=int(tp)\n",
        "    fp=int(fp)\n",
        "    fn=int(fn)\n",
        "\n",
        "    # Special cases: empty data or full agreement\n",
        "    if fn == 0 and fp == 0:\n",
        "        return 1.0\n",
        "\n",
        "    return 2. * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) +\n",
        "                                       (tp + fp) * (fp + tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU2J9flvkz1R"
      },
      "outputs": [],
      "source": [
        "def f1_score(labels_true,labels_pred): \n",
        "    '''safer implementation of ari score calculation'''\n",
        "    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
        "    tn=int(tn)\n",
        "    tp=int(tp)\n",
        "    fp=int(fp)\n",
        "    fn=int(fn)\n",
        "\n",
        "    precision= tp/(tp+fp)\n",
        "    recall= tp/(tp+fn)\n",
        "\n",
        "    # Special cases: empty data or full agreement\n",
        "    if fn == 0 and fp == 0:\n",
        "        return 1.0\n",
        "\n",
        "    return 2. * precision * recall / (precision+recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44mhc8DKh4b0"
      },
      "outputs": [],
      "source": [
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oXsB1KuSIFn"
      },
      "outputs": [],
      "source": [
        "def min_max(data):\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(data)\n",
        "  return scaler.transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-SyZIKXJGyv"
      },
      "source": [
        "## Ploting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHpKl9ZuK0DO"
      },
      "outputs": [],
      "source": [
        "def plot_bivariate_data(X, title):\n",
        "  fig = plt.figure(figsize=[8, 4])\n",
        "  gs = fig.add_gridspec(2, 2)\n",
        "  ax = fig.add_subplot(gs[:, 1])\n",
        "  ax.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
        "           markeredgewidth=0)\n",
        "  plt.xlabel('Feature 1')\n",
        "  plt.ylabel('Feature 2')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Wg1jbguVOO"
      },
      "outputs": [],
      "source": [
        "def plot_contours(data, means, covs,labels, title, xa=[-12,12],ya=[-12,12]):\n",
        "    \"\"\"visualize the gaussian components over the data\"\"\"\n",
        "    plt.figure()\n",
        "    cmap = plt.cm.get_cmap('tab10')\n",
        "    plt.scatter(data[:, 0], data[:, 1],c=labels, cmap=cmap, s=40 ,alpha=0.4)\n",
        "    \n",
        "\n",
        "    delta = 0.025\n",
        "    if len(means)>0:\n",
        "      k = means.shape[0]\n",
        "    else:\n",
        "      k=0\n",
        "    x = np.arange(xa[0], xa[1], delta)\n",
        "    y = np.arange(ya[0], ya[1], delta)\n",
        "    x_grid, y_grid = np.meshgrid(x, y)\n",
        "    coordinates = np.array([x_grid.ravel(), y_grid.ravel()]).T\n",
        "\n",
        "    col = ['cyan', 'red', 'indigo','blue','white']\n",
        "    for i in range(k):\n",
        "        mean = means[i]\n",
        "        cov = covs[i]\n",
        "        z_grid = multivariate_normal(mean, cov).pdf(coordinates).reshape(x_grid.shape)\n",
        "        plt.contour(x_grid, y_grid, z_grid, colors = col[i])\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_8ExRVGJ8g_"
      },
      "source": [
        "# KL Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpZkJ6rYBTuH"
      },
      "outputs": [],
      "source": [
        "def kl_mvn(m0, S0, m1, S1):\n",
        "    \"\"\"\n",
        "    Kullback-Liebler divergence from Gaussian pm,pv to Gaussian qm,qv.\n",
        "    Also computes KL divergence from a single Gaussian pm,pv to a set\n",
        "    of Gaussians qm,qv.\n",
        "    \n",
        "\n",
        "    From wikipedia\n",
        "    KL( (m0, S0) || (m1, S1))\n",
        "         = .5 * ( tr(S1^{-1} S0) + log |S1|/|S0| + \n",
        "                  (m1 - m0)^T S1^{-1} (m1 - m0) - N )\n",
        "    \"\"\"\n",
        "    \n",
        "    # store inv diag covariance of S1 and diff between means\n",
        "    N = m0.shape[0]\n",
        "    iS1 = np.linalg.inv(S1)\n",
        "    #way to calculate the determinant to avoid underflow and overflow\n",
        "    sign,logdetS0=np.linalg.slogdet(S0)\n",
        "   # detS0=sign * np.exp(logdetS0)\n",
        "    sign,logdetS1=np.linalg.slogdet(S1)\n",
        "    #detS1=sign * np.exp(logdetS1)\n",
        "\n",
        "    diff = m1 - m0\n",
        "    # kl is made of three terms\n",
        "    tr_term   = np.trace(iS1 @ S0)\n",
        "    det_term  = logdetS1-logdetS0\n",
        "   # print(det_term) #np.sum(np.log(S1)) - np.sum(np.log(S0))\n",
        "    quad_term = diff.T @ np.linalg.inv(S1) @ diff #np.sum( (diff*diff) * iS1, axis=1)\n",
        "    #print(tr_term,det_term,quad_term)\n",
        "    return .5 * (tr_term + det_term + quad_term - N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-vu1vngTem"
      },
      "outputs": [],
      "source": [
        "def js_mvn(m0, s0, m1, s1):\n",
        "  m2=(m0+m1)/2\n",
        "  s2=s0+s1\n",
        "  return (kl_mvn(m0, s0, m2, s2) + kl_mvn(m1, s1, m2, s2))/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt2XkhWJFQZg"
      },
      "outputs": [],
      "source": [
        "def KL_matrix(m0,S0,m1,S1):\n",
        "  k0=m0.shape[0]\n",
        "  k1=m1.shape[0]\n",
        "  M=np.zeros((k0,k1))\n",
        "  for i in range(k0):\n",
        "    for j in range(k1):\n",
        "      M[i,j]=kl_mvn(m0[i],S0[i], m1[j], S1[j])\n",
        "  return M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aBOiUl1KW5O"
      },
      "source": [
        "# Incremental EM for GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_h9OBLGe7O0"
      },
      "outputs": [],
      "source": [
        "def trainGMM(data_x,n_components,dim, random_state,covariance_type='full',max_iter = 300,tol=1e-08):\n",
        "  gmm = GMM(n_components=n_components, covariance_type=covariance_type, random_state=random_state, max_iter=max_iter,tol=tol,verbose=0)\n",
        "  gmm.fit(data_x)\n",
        "  return gmm.weights_,gmm.means_,gmm.covariances_,gmm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHwzB7XC-H_x"
      },
      "outputs": [],
      "source": [
        "def merge_two(n0,w0,m0,s0,n1,w1,m1,s1):\n",
        "    new_mean=(n0*w0*m0+n1*w1*m1)/(n0*w0+n1*w1)\n",
        "    new_weight=w0+w1#(n0*w0+n1*w1)/(n0+n1)\n",
        "    s1=(n0*w0*s0+n1*w1*s1)/(n0*w0+n1*w1)\n",
        "    sw=n0*w0*np.outer(np.transpose(m0),m0)+n1*w1*np.outer(np.transpose(m1),m1)\n",
        "    s2=sw/(n0*w0+n1*w1)\n",
        "    sub3=np.outer(np.transpose(new_mean),new_mean)\n",
        "    new_cov=s1+s2-sub3\n",
        "    n=n0\n",
        "    return new_weight,new_mean,new_cov,n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGBIsbgzL1Vh"
      },
      "outputs": [],
      "source": [
        "def incGMM(data,dim, n_components, increments_number, random_state, covariance_type='full',max_iter = 100,tol=1e-03, true_lables=[], incPrint=True):\n",
        "  size_increments=int(len(data)/increments_number)\n",
        "  clus_increment_size=int(size_increments/n_components)\n",
        "  assignments=[]\n",
        "  increments=[]\n",
        "  weights=[]\n",
        "  means=[]\n",
        "  inc_labels=[]\n",
        "  covariances=[]\n",
        "  n0=0\n",
        "  gmm = 0\n",
        "  all_covs=[]\n",
        "  all_means=[]\n",
        "  all_weights=[]\n",
        "  all_n=[]\n",
        "  for i in range(increments_number):\n",
        "      s=[i for j in range(size_increments)]\n",
        "      inc_labels.append(s)\n",
        "      if i == (increments_number - 1):\n",
        "        inc = data[i*size_increments:data.shape[0],:]\n",
        "      else:\n",
        "        inc = data[i*size_increments:size_increments*(i+1),:]\n",
        "      increments.append(inc)\n",
        "      w,m,covs,gmm = trainGMM(inc,n_components,dim, covariance_type='full',random_state=random_state,max_iter = 500,tol=1e-06)\n",
        "      \n",
        "      all_means.append(m)\n",
        "      all_covs.append(covs)\n",
        "      all_weights.append(w)\n",
        "      e=len(data)*np.ones((w.shape[0]))\n",
        "      all_n.append(e)\n",
        "      n1=len(inc)\n",
        "      n0=n0+n1\n",
        "      if incPrint and (len(true_lables)>0) and data.shape[1]==2:\n",
        "        plot_contours(inc,means,covariances,true_lables[i*size_increments:size_increments*(i+1)], \"increments_true\")\n",
        "        plt.savefig(\"inc_true{}.png\".format(i))\n",
        "  if incPrint and data.shape[1]==2:\n",
        "    increments, inc_labels = np.array(increments).reshape(data.shape[0],dim) , np.array(inc_labels).reshape(data.shape[0])\n",
        "    plot_contours(increments,[],[],inc_labels, \"increments_partition\")\n",
        "  all_weights=np.array(all_weights)\n",
        "  all_means=np.array(all_means)\n",
        "  all_covs=np.array(all_covs)\n",
        "  all_n=np.array(all_n)\n",
        "  all_weights=all_weights.reshape(all_weights.shape[0]*all_weights.shape[1])/increments_number\n",
        "  all_n=all_n.reshape(all_n.shape[0]*all_n.shape[1])\n",
        "  all_means=all_means.reshape(all_means.shape[0]*all_means.shape[1],all_means.shape[2])\n",
        "  all_covs=all_covs.reshape(all_covs.shape[0]*all_covs.shape[1],all_covs.shape[2],all_covs.shape[3])\n",
        "  #all_n=np.array([int(j) for j in all_n])\n",
        "\n",
        "  return all_weights,all_means,all_covs,all_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVq_mcUuKoS1"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCewyBVqSaGR"
      },
      "outputs": [],
      "source": [
        "def getFinalGmm(data, means, covariances, weights, true_labels = [], plot=True):\n",
        "    gmm=GMM(n_components=weights.shape[0],covariance_type='full',max_iter=1)\n",
        "    gmm.means_=means\n",
        "    gmm.covariances_=covariances\n",
        "    gmm.weights_=weights\n",
        "    precisions_cholesky = np.linalg.inv(la.cholesky(covariances))\n",
        "    gmm.precisions_cholesky_= np.array([np.transpose(p) for p in precisions_cholesky])\n",
        "    assign = gmm.predict(data)\n",
        "    if plot and len(true_labels>0) and data.shape[1]==2:\n",
        "        plot_contours(data,means,covariances,assign, \"increments\")\n",
        "    return gmm, assign\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RM3Wc5NRA6S"
      },
      "source": [
        "## R generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vQy5RIRRF2h"
      },
      "outputs": [],
      "source": [
        "def read_info_file(path):\n",
        "    words = [\"sepVal\", \"Number of clusters\", \"Number of dimensions\", \"Number of data points\", \"Number of outliers\"]\n",
        "    N, dim, number_comp, sepVal, outliers = (0,0,0,0,0)\n",
        "    with open(path, 'r') as fp:\n",
        "        # read all lines using readline()\n",
        "        lines = fp.readlines()\n",
        "        for row in lines:\n",
        "            for word in words:\n",
        "                if row.find(word) != -1:\n",
        "                    x = row.split(' ')[-1]\n",
        "                    if word == words[0]:\n",
        "                        sepVal=float(x)\n",
        "                    elif word == words[1]:\n",
        "                        number_comp=int(x)\n",
        "                    elif word == words[2]:\n",
        "                        dim=int(x)\n",
        "                    elif word == words[3]:\n",
        "                        N=int(x)\n",
        "                    else:\n",
        "                        outliers=float(x)\n",
        "                        \n",
        "    return N, dim, number_comp, sepVal, outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4fLLiddRdYG",
        "outputId": "8909acd5-1e79-4787-8012-6855986df3ed",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/meriem/Documents/data/component/1/\n",
            "/home/meriem/Documents/data/component/1/big-16000-0-30-2.mat\n",
            "2\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "time  ======= 2.8940303325653076\n",
            "3\n",
            "max  0.9660411430654066 min  0.9660411430654066  ari  0.9660411430654066\n",
            "sum  1.0\n",
            "max  0.9660411430654066 min  0.9660411430654066  ari  0.9660411430654066\n",
            "sum  1.0\n",
            "max  0.9660411430654066 min  0.9660411430654066  ari  0.9660411430654066\n",
            "sum  0.9999999999999999\n",
            "max  0.9660411430654066 min  0.9660411430654066  ari  0.9660411430654066\n",
            "sum  1.0\n",
            "max  0.9660411430654066 min  0.9660411430654066  ari  0.9660411430654066\n",
            "sum  1.0\n",
            "time  ======= 2.8904380798339844\n",
            "4\n",
            "max  0.9655497373506533 min  0.9655497373506533  ari  0.9655497373506533\n",
            "sum  1.0\n",
            "max  0.9655497373506533 min  0.9655497373506533  ari  0.9655497373506533\n",
            "sum  1.0\n",
            "max  0.9655497373506533 min  0.9655497373506533  ari  0.9655497373506533\n",
            "sum  1.0\n",
            "max  0.9655497373506533 min  0.9655497373506533  ari  0.9655497373506533\n",
            "sum  1.0\n",
            "max  0.9655497373506533 min  0.9655497373506533  ari  0.9655497373506533\n",
            "sum  1.0\n",
            "time  ======= 5.351033449172974\n",
            "5\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  0.9999999999999999\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  0.9999999999999999\n",
            "time  ======= 6.731637001037598\n",
            "6\n",
            "max  0.9665326737871889 min  0.9665326737871889  ari  0.9665326737871889\n",
            "sum  1.0\n",
            "max  0.9665326737871889 min  0.9665326737871889  ari  0.9665326737871889\n",
            "sum  1.0\n",
            "max  0.9665326737871889 min  0.9665326737871889  ari  0.9665326737871889\n",
            "sum  1.0\n",
            "max  0.9665326737871889 min  0.9665326737871889  ari  0.9665326737871889\n",
            "sum  1.0\n",
            "max  0.9665326737871889 min  0.9665326737871889  ari  0.9665326737871889\n",
            "sum  1.0\n",
            "time  ======= 6.728897333145142\n",
            "7\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  0.9999999999999999\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "max  0.9657954245816189 min  0.9657954245816189  ari  0.9657954245816189\n",
            "sum  1.0\n",
            "time  ======= 9.031639575958252\n",
            "8\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n",
            "max  0.9660411430646106 min  0.9660411430646106  ari  0.9660411430646106\n",
            "sum  1.0\n"
          ]
        }
      ],
      "source": [
        "mainDir=\"/home/meriem/Documents/data/component/\"\n",
        "i=0\n",
        "import time\n",
        "time_eco=0\n",
        "start_time = time.time()\n",
        "with open(mainDir+'results_merging.csv', 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    header=[\"DatasetNumber\", \"size\", \"dim\", \"Percentage of increment\", \"cluster size\", \"number of clusters\", \"sepVal\", \"Percentage of outliers\", \"ari\",\"min\",\"max\"]\n",
        "    writer.writerow(header)\n",
        "   \n",
        "    for s in ['1','2','3','4']:\n",
        "            data_file_chemin = mainDir+s+'/'\n",
        "            print(data_file_chemin)\n",
        "            files =os.listdir(data_file_chemin)\n",
        "            if(len(files)>0):\n",
        "                data_file =  data_file_chemin+[f for f in files if f.endswith(\".mat\")][0]\n",
        "                print(data_file)\n",
        "                labels_file =data_file_chemin+ [f for f in files if f.endswith(\".mem\")][0]\n",
        "                info_file = data_file_chemin+[f for f in files if (f.endswith(\".log\") and not f.endswith(\"info.log\"))][0]\n",
        "                N, dim, number_comp, sepVal, outliers = read_info_file(info_file)\n",
        "\n",
        "                data=pd.read_csv(data_file,sep=\" \",header=None)\n",
        "                data_labels=pd.read_csv(labels_file,header=None)\n",
        "                data_labels.rename(columns = {0:2}, inplace = True)\n",
        "                toute=pd.concat([data,data_labels],axis=1) \n",
        "                # sctt_plt = sns.scatterplot(data=toute, x=0, y=1, hue=2)\n",
        "                # fig = sctt_plt.get_figure()\n",
        "                # fig.savefig(dataset_dir+\"out.png\")\n",
        "                # plt.clf()\n",
        "                i=i+1\n",
        "                toute=pd.concat([data,data_labels],axis=1)\n",
        "                toute=toute.dropna()\n",
        "                toute=toute.to_numpy()\n",
        "                np.unique(data_labels.to_numpy(),return_counts=True)\n",
        "                labels_true=data_labels.to_numpy().reshape(-1)\n",
        "                data=data.to_numpy()\n",
        "                data=min_max(data)\n",
        "                #***********************************************GMM***********************************************\n",
        "                trials=5\n",
        "                for i in range(2,19):\n",
        "                  print(i)\n",
        "                  metrics=0\n",
        "                  all_gmm=GMM(n_components=number_comp, covariance_type='full',max_iter = 300,tol=10e-8)\n",
        "                  all_gmm.fit(data)\n",
        "                  assign_all = all_gmm.predict(data)\n",
        "                  metrics = np.zeros(len(header)-8)\n",
        "                  minim=1\n",
        "                  maxim=0\n",
        "                  ari_all=0\n",
        "                  debut=time.time()\n",
        "                  for k in range(trials):\n",
        "                    all_weights,all_means,all_covs,all_n=incGMM(data,dim, number_comp, i, k, \n",
        "                                                                covariance_type='full',max_iter = 300,tol=1e-08, \n",
        "                                                                true_lables=labels_true, incPrint=False)  \n",
        "                    while (all_weights.shape[0]!=number_comp):\n",
        "                            #print(all_means)\n",
        "                            B=calculerM(all_weights,all_means,all_covs,all_n)\n",
        "                            #print(B)\n",
        "                            all_weights,all_means,all_covs,all_n=merge_operation(all_weights,all_means,all_covs,all_n,B)\n",
        "                          #  print('sum ',sum(all_weights))\n",
        "                            gmm, assign=getFinalGmm(data, all_means, all_covs, all_weights,labels_true , plot=False)\n",
        "                    ari_one=ari(assign,labels_true)\n",
        "                    ari_all=ari_one+ari_all\n",
        "                    if (ari_one<minim):\n",
        "                            minim=ari_one\n",
        "                    if (ari_one>maxim):\n",
        "                            maxim=ari_one\n",
        "                  \n",
        "                    print('max ',maxim,'min ',minim,\" ari \",ari_one)\n",
        "                    print('sum ',sum(all_weights))\n",
        "                  fin=time.time()-debut\n",
        "                  time_eco=fin\n",
        "                            \n",
        "\n",
        "                        \n",
        "                  metric = ari_all/trials\n",
        "                  #print(metric)\n",
        "                  info = [data_file, N, dim,i, int(N/number_comp), number_comp, sepVal, outliers]\n",
        "                  info.extend([metric])\n",
        "                  info.extend([minim])\n",
        "                  info.extend([maxim])\n",
        "                  info.extend([time_eco])\n",
        "                  writer.writerow(info)\n",
        "                  print('time  =======',time_eco)\n",
        "                  time_eco=0\n",
        "                 # print('number of inc ',i, '  ari  ',ari_all/trials)\n",
        "                      #  print(all_means,all_n)\n",
        "                    \n",
        "                #calculerM(all_weights,all_means,all_covs,all_n)\n",
        "    file.close()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-g2Ppu4ZiOc",
        "outputId": "46c3b142-def4-43ba-9678-999cc41aab3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_weights\n",
        "sum(all_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf-RlxsyZiOc",
        "outputId": "a82c0df1-483b-4048-ac36-21e3ea86ce2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialization 0\n",
            "Initialization converged: True\t time lapse 14.54879s\t ll 29.20863\n"
          ]
        }
      ],
      "source": [
        "all_gmm=GMM(n_components=number_comp, covariance_type='full',max_iter = 500,tol=10e-8,verbose=2)\n",
        "all_gmm.fit(data)\n",
        "assign_all = all_gmm.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxyUC0tJZiOc",
        "outputId": "3b1f2635-b85f-403b-d913-31c6e5b5a870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.805213430286735"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ari(assign_all,labels_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V_jZWT3ZiOd",
        "outputId": "d3e8d67e-2600-4973-9218-15120011e786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9660411430677946"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ari(assign,labels_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQQSbxGOZiOd"
      },
      "outputs": [],
      "source": [
        "def calculerM(all_weights,all_means,all_covs,all_n):\n",
        "    n=all_weights.shape[0]\n",
        "    A=np.zeros((n,n))\n",
        "    for i in range(n):\n",
        "        \n",
        "        for j in range(n):\n",
        "            if i<j:\n",
        "                  m0=all_means[i]\n",
        "                  m1=all_means[j]\n",
        "                  S0=all_covs[i]\n",
        "                  S1=all_covs[j]\n",
        "                  A[i,j]=kl_mvn(m0, S0, m1, S1)\n",
        "            else:\n",
        "                A[i,j]=float('inf')\n",
        "    return A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQPs1NuZiOd",
        "outputId": "1f2ad999-99b8-4ad7-f814-7736d1ff3ece"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for axis 0 with size 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_12317/3301071794.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m#  print(all_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_covs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_covs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_covs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_12317/3301071794.py\u001b[0m in \u001b[0;36mmerge_operation\u001b[0;34m(all_weights, all_means, all_covs, all_n, A)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mm0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mS0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_covs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mS1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_covs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
          ]
        }
      ],
      "source": [
        "def merge_operation(all_weights,all_means,all_covs,all_n,A):\n",
        "    #print(np.argmin(B))\n",
        "    ind=np.argwhere(B == np.min(B))[0]\n",
        "    #get the indexes of min value\n",
        "    \n",
        "    i=ind[0]\n",
        "    j=ind[1]\n",
        "    m0=all_means[i]\n",
        "    m1=all_means[j]\n",
        "    S0=all_covs[i]\n",
        "    S1=all_covs[j]\n",
        "    w0=all_weights[i]\n",
        "    w1=all_weights[j]\n",
        "    n0=all_n[i]\n",
        "    n1=all_n[j]\n",
        "    #print('all_n',sum(all_n*all_weights))\n",
        "    #delete the merged components\n",
        "    if i<j:\n",
        "        k=j\n",
        "        j=i\n",
        "        i=k\n",
        "    A=np.delete(A,i,axis=0)\n",
        "    A=np.delete(A,j,axis=0)\n",
        "    A=np.delete(A,i,axis=1)\n",
        "    A=np.delete(A,j,axis=1)\n",
        "    all_covs=np.delete(all_covs,i,axis=0)\n",
        "    all_covs=np.delete(all_covs,j,axis=0)\n",
        "    all_means=np.delete(all_means,i,axis=0)\n",
        "    all_means=np.delete(all_means,j,axis=0)\n",
        "\n",
        "    all_weights=np.delete(all_weights,i,axis=0)\n",
        "    all_weights=np.delete(all_weights,j,axis=0)\n",
        "\n",
        "    all_n=np.delete(all_n,i,axis=0)\n",
        "    all_n=np.delete(all_n,j,axis=0)\n",
        "    \n",
        "\n",
        "        \n",
        "    \n",
        "    \n",
        "        \n",
        "    new_weight,new_mean,new_cov,n=merge_two(n0,w0,m0,S0,n1,w1,m1,S1)  \n",
        "    #n=n0+n1\n",
        "#     all_weight=all_weights*all_n /sum(all_n*all_weights)\n",
        "#     print('sum weights',sum(all_weight))\n",
        "    \n",
        "\n",
        "    all_means=np.append(all_means, [new_mean],axis=0)\n",
        "    all_covs=np.append(all_covs,[new_cov],axis=0)\n",
        "    all_weights=np.append(all_weights,[new_weight],axis=0)\n",
        "    all_n=np.append(all_n,[n],axis=0)\n",
        "  #  print(all_weights)\n",
        "    return all_weights,all_means,all_covs,all_n\n",
        "all_weights,all_means,all_covs,all_n=merge_operation(all_weights,all_means,all_covs,all_n,B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hQfsydrZiOe",
        "outputId": "4c7fa1dc-5310-40b2-898b-fb13d39eb615"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 30 and the array at index 1 has size 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_12317/27003185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_covs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4815\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 30 and the array at index 1 has size 2"
          ]
        }
      ],
      "source": [
        "np.append(all_means,[[2,2]],axis=0)\n",
        "all_covs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8L7WTT3ZiOe"
      },
      "outputs": [],
      "source": [
        "np.argwhere(B == np.min(B))[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzhMj1HLZiOe"
      },
      "outputs": [],
      "source": [
        "sum(all_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKDWUxX_ZiOe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}